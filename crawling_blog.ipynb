{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수집된 링크파일불러서 블로그 본문내용 크롤링(python3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고자료: https://www.youtube.com/watch?v=cB8bRCTgqJ8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일명만 바꾸어 총 6번 수행(넷플릭스 3, 왓챠 3)\n",
    "#넷플릭스 첫번째\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# 링크가 들어있는 파일 열기\n",
    "cf=open(\"./bigData/bloglink_netflix1.json\",\"r\")\n",
    "lines=cf.readlines()  #전체 링크가 들어있는 리스트\n",
    "cf.close()\n",
    "\n",
    "# 링크에서 맨 뒤에 \\n 제거하기\n",
    "links=list()  #제거한 링크를 담을 리스트\n",
    "pattern='[\\n]'\n",
    "repl=''\n",
    "for i in lines:\n",
    "    link=re.sub(pattern=pattern, repl=repl, string=i)\n",
    "    links.append(link)\n",
    "\n",
    "file=open(\"./bigData/blogdetails_netflix.json\",\"a\",-1,\"utf-8\")\n",
    "\n",
    "# 링크 하나씩 가져다가 크롤링\n",
    "for line in links:\n",
    "    link=line   \n",
    "    \n",
    "    # 링크를 크롤링-본문만 크롤링\n",
    "    post_code=requests.get(link)\n",
    "    post_text=post_code.text\n",
    "    post_soup=BeautifulSoup(post_text, 'lxml')\n",
    "\n",
    "    for mainFrame in post_soup.select('iframe#mainFrame'):\n",
    "        blog_post_url=\"http://blog.naver.com\" + mainFrame.get('src')\n",
    "        blog_post_code=requests.get(blog_post_url)\n",
    "        blog_post_text=blog_post_code.text\n",
    "        blog_post_soup=BeautifulSoup(blog_post_text, 'lxml')  \n",
    "        for blog_post_content in blog_post_soup.select('div.se-main-container'):\n",
    "            blog_post_content_text=blog_post_content.get_text()\n",
    "            blog_post_full_contents=str(blog_post_content_text)\n",
    "            blog_post_full_contents=blog_post_full_contents.replace(\"\\n\\n\", \"\\n\")\n",
    "            file.write(blog_post_full_contents)\n",
    "        file.write(\"-----\")    \n",
    "file.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일명만 바꾸어 총 6번 수행(넷플릭스 3, 왓챠 3)\n",
    "#넷플릭스 첫번째\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# 링크가 들어있는 파일 열기\n",
    "cf=open(\"./bigData/bloglink_netflix2.json\",\"r\")\n",
    "lines=cf.readlines()  #전체 링크가 들어있는 리스트\n",
    "cf.close()\n",
    "\n",
    "# 링크에서 맨 뒤에 \\n 제거하기\n",
    "links=list()  #제거한 링크를 담을 리스트\n",
    "pattern='[\\n]'\n",
    "repl=''\n",
    "for i in lines:\n",
    "    link=re.sub(pattern=pattern, repl=repl, string=i)\n",
    "    links.append(link)\n",
    "\n",
    "file=open(\"./bigData/blogdetails_netflix.json\",\"a\",-1,\"utf-8\")\n",
    "\n",
    "# 링크 하나씩 가져다가 크롤링\n",
    "for line in links:\n",
    "    link=line   \n",
    "    \n",
    "    # 링크를 크롤링-본문만 크롤링\n",
    "    post_code=requests.get(link)\n",
    "    post_text=post_code.text\n",
    "    post_soup=BeautifulSoup(post_text, 'lxml')\n",
    "\n",
    "    for mainFrame in post_soup.select('iframe#mainFrame'):\n",
    "        blog_post_url=\"http://blog.naver.com\" + mainFrame.get('src')\n",
    "        blog_post_code=requests.get(blog_post_url)\n",
    "        blog_post_text=blog_post_code.text\n",
    "        blog_post_soup=BeautifulSoup(blog_post_text, 'lxml')  \n",
    "        for blog_post_content in blog_post_soup.select('div.se-main-container'):\n",
    "            blog_post_content_text=blog_post_content.get_text()\n",
    "            blog_post_full_contents=str(blog_post_content_text)\n",
    "            blog_post_full_contents=blog_post_full_contents.replace(\"\\n\\n\", \"\\n\")\n",
    "            file.write(blog_post_full_contents)\n",
    "        file.write(\"-----\")    \n",
    "file.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일명만 바꾸어 총 6번 수행(넷플릭스 3, 왓챠 3)\n",
    "#넷플릭스 세번째\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# 링크가 들어있는 파일 열기\n",
    "cf=open(\"./bigData/bloglink_netflix3.json\",\"r\")\n",
    "lines=cf.readlines()  #전체 링크가 들어있는 리스트\n",
    "cf.close()\n",
    "\n",
    "# 링크에서 맨 뒤에 \\n 제거하기\n",
    "links=list()  #제거한 링크를 담을 리스트\n",
    "pattern='[\\n]'\n",
    "repl=''\n",
    "for i in lines:\n",
    "    link=re.sub(pattern=pattern, repl=repl, string=i)\n",
    "    links.append(link)\n",
    "\n",
    "file=open(\"./bigData/blogdetails_netflix.json\",\"a\",-1,\"utf-8\")\n",
    "\n",
    "# 링크 하나씩 가져다가 크롤링\n",
    "for line in links:\n",
    "    link=line   \n",
    "    \n",
    "    # 링크를 크롤링-본문만 크롤링\n",
    "    post_code=requests.get(link)\n",
    "    post_text=post_code.text\n",
    "    post_soup=BeautifulSoup(post_text, 'lxml')\n",
    "\n",
    "    for mainFrame in post_soup.select('iframe#mainFrame'):\n",
    "        blog_post_url=\"http://blog.naver.com\" + mainFrame.get('src')\n",
    "        blog_post_code=requests.get(blog_post_url)\n",
    "        blog_post_text=blog_post_code.text\n",
    "        blog_post_soup=BeautifulSoup(blog_post_text, 'lxml')  \n",
    "        for blog_post_content in blog_post_soup.select('div.se-main-container'):\n",
    "            blog_post_content_text=blog_post_content.get_text()\n",
    "            blog_post_full_contents=str(blog_post_content_text)\n",
    "            blog_post_full_contents=blog_post_full_contents.replace(\"\\n\\n\", \"\\n\")\n",
    "            file.write(blog_post_full_contents)\n",
    "        file.write(\"-----\")    \n",
    "file.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일명만 바꾸어 총 6번 수행(넷플릭스 3, 왓챠 3)\n",
    "#왓챠 첫번째\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# 링크가 들어있는 파일 열기\n",
    "cf=open(\"./bigData/bloglink_watcha1.json\",\"r\")\n",
    "lines=cf.readlines()  #전체 링크가 들어있는 리스트\n",
    "cf.close()\n",
    "\n",
    "# 링크에서 맨 뒤에 \\n 제거하기\n",
    "links=list()  #제거한 링크를 담을 리스트\n",
    "pattern='[\\n]'\n",
    "repl=''\n",
    "for i in lines:\n",
    "    link=re.sub(pattern=pattern, repl=repl, string=i)\n",
    "    links.append(link)\n",
    "\n",
    "file=open(\"./bigData/blogdetails_watcha.json\",\"a\",-1,\"utf-8\")\n",
    "\n",
    "# 링크 하나씩 가져다가 크롤링\n",
    "for line in links:\n",
    "    link=line   \n",
    "    \n",
    "    # 링크를 크롤링-본문만 크롤링\n",
    "    post_code=requests.get(link)\n",
    "    post_text=post_code.text\n",
    "    post_soup=BeautifulSoup(post_text, 'lxml')\n",
    "\n",
    "    for mainFrame in post_soup.select('iframe#mainFrame'):\n",
    "        blog_post_url=\"http://blog.naver.com\" + mainFrame.get('src')\n",
    "        blog_post_code=requests.get(blog_post_url)\n",
    "        blog_post_text=blog_post_code.text\n",
    "        blog_post_soup=BeautifulSoup(blog_post_text, 'lxml')  \n",
    "        for blog_post_content in blog_post_soup.select('div.se-main-container'):\n",
    "            blog_post_content_text=blog_post_content.get_text()\n",
    "            blog_post_full_contents=str(blog_post_content_text)\n",
    "            blog_post_full_contents=blog_post_full_contents.replace(\"\\n\\n\", \"\\n\")\n",
    "            file.write(blog_post_full_contents)\n",
    "        file.write(\"-----\")    \n",
    "file.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일명만 바꾸어 총 6번 수행(넷플릭스 3, 왓챠 3)\n",
    "#왓챠 두번째\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# 링크가 들어있는 파일 열기\n",
    "cf=open(\"./bigData/bloglink_watcha2.json\",\"r\")\n",
    "lines=cf.readlines()  #전체 링크가 들어있는 리스트\n",
    "cf.close()\n",
    "\n",
    "# 링크에서 맨 뒤에 \\n 제거하기\n",
    "links=list()  #제거한 링크를 담을 리스트\n",
    "pattern='[\\n]'\n",
    "repl=''\n",
    "for i in lines:\n",
    "    link=re.sub(pattern=pattern, repl=repl, string=i)\n",
    "    links.append(link)\n",
    "\n",
    "file=open(\"./bigData/blogdetails_watcha.json\",\"a\",-1,\"utf-8\")\n",
    "\n",
    "# 링크 하나씩 가져다가 크롤링\n",
    "for line in links:\n",
    "    link=line   \n",
    "    \n",
    "    # 링크를 크롤링-본문만 크롤링\n",
    "    post_code=requests.get(link)\n",
    "    post_text=post_code.text\n",
    "    post_soup=BeautifulSoup(post_text, 'lxml')\n",
    "\n",
    "    for mainFrame in post_soup.select('iframe#mainFrame'):\n",
    "        blog_post_url=\"http://blog.naver.com\" + mainFrame.get('src')\n",
    "        blog_post_code=requests.get(blog_post_url)\n",
    "        blog_post_text=blog_post_code.text\n",
    "        blog_post_soup=BeautifulSoup(blog_post_text, 'lxml')  \n",
    "        for blog_post_content in blog_post_soup.select('div.se-main-container'):\n",
    "            blog_post_content_text=blog_post_content.get_text()\n",
    "            blog_post_full_contents=str(blog_post_content_text)\n",
    "            blog_post_full_contents=blog_post_full_contents.replace(\"\\n\\n\", \"\\n\")\n",
    "            file.write(blog_post_full_contents)\n",
    "        file.write(\"-----\")    \n",
    "file.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일명만 바꾸어 총 6번 수행(넷플릭스 3, 왓챠 3)\n",
    "#왓챠 세번째\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# 링크가 들어있는 파일 열기\n",
    "cf=open(\"./bigData/bloglink_watcha3.json\",\"r\")\n",
    "lines=cf.readlines()  #전체 링크가 들어있는 리스트\n",
    "cf.close()\n",
    "\n",
    "# 링크에서 맨 뒤에 \\n 제거하기\n",
    "links=list()  #제거한 링크를 담을 리스트\n",
    "pattern='[\\n]'\n",
    "repl=''\n",
    "for i in lines:\n",
    "    link=re.sub(pattern=pattern, repl=repl, string=i)\n",
    "    links.append(link)\n",
    "\n",
    "file=open(\"./bigData/blogdetails_watcha.json\",\"a\",-1,\"utf-8\")\n",
    "\n",
    "# 링크 하나씩 가져다가 크롤링\n",
    "for line in links:\n",
    "    link=line   \n",
    "    \n",
    "    # 링크를 크롤링-본문만 크롤링\n",
    "    post_code=requests.get(link)\n",
    "    post_text=post_code.text\n",
    "    post_soup=BeautifulSoup(post_text, 'lxml')\n",
    "\n",
    "    for mainFrame in post_soup.select('iframe#mainFrame'):\n",
    "        blog_post_url=\"http://blog.naver.com\" + mainFrame.get('src')\n",
    "        blog_post_code=requests.get(blog_post_url)\n",
    "        blog_post_text=blog_post_code.text\n",
    "        blog_post_soup=BeautifulSoup(blog_post_text, 'lxml')  \n",
    "        for blog_post_content in blog_post_soup.select('div.se-main-container'):\n",
    "            blog_post_content_text=blog_post_content.get_text()\n",
    "            blog_post_full_contents=str(blog_post_content_text)\n",
    "            blog_post_full_contents=blog_post_full_contents.replace(\"\\n\\n\", \"\\n\")\n",
    "            file.write(blog_post_full_contents)\n",
    "        file.write(\"-----\")    \n",
    "file.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
